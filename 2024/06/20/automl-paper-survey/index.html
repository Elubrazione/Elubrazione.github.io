<!DOCTYPE html>
<html lang="cn">
    <!-- title -->


    

<!-- keywords -->



<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="author" content="Elubrazione">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="Elubrazione">
    
        <meta name="keywords" content="elubrazione,lingching">
    
    <meta name="description" content="">
    <meta name="description" content="。">
<meta property="og:type" content="article">
<meta property="og:title" content="automl-paper-survey">
<meta property="og:url" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="。">
<meta property="og:locale">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240826125411325.png">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240627163937730.png">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240826151400164.png">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240826155923846.png">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240826160706655.png">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240826170638588.png">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240826223328862.png">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240826203654749.png">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240627164911129.png">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240627162942070.png">
<meta property="og:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240827201109456.png">
<meta property="article:published_time" content="2024-06-20T09:12:40.000Z">
<meta property="article:modified_time" content="2024-09-07T03:47:52.332Z">
<meta property="article:author" content="Elubrazione">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://elubrazione.github.io/2024/06/20/automl-paper-survey/image-20240826125411325.png">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <link rel="icon" href="/assets/favicon1.ico">
    
    <title>automl-paper-survey · TUNIVERSE</title>
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
    (function (w) {
        'use strict'
        // rel=preload support test
        if (!w.loadCSS) {
            w.loadCSS = function () {}
        }
        // define on the loadCSS obj
        var rp = (loadCSS.relpreload = {})
        // rel=preload feature support test
        // runs once and returns a function for compat purposes
        rp.support = (function () {
            var ret
            try {
                ret = w.document.createElement('link').relList.supports('preload')
            } catch (e) {
                ret = false
            }
            return function () {
                return ret
            }
        })()

        // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
        // then change that media back to its intended value on load
        rp.bindMediaToggle = function (link) {
            // remember existing media attr for ultimate state, or default to 'all'
            var finalMedia = link.media || 'all'

            function enableStylesheet() {
                link.media = finalMedia
            }

            // bind load handlers to enable media
            if (link.addEventListener) {
                link.addEventListener('load', enableStylesheet)
            } else if (link.attachEvent) {
                link.attachEvent('onload', enableStylesheet)
            }

            // Set rel and non-applicable media type to start an async request
            // note: timeout allows this to happen async to let rendering continue in IE
            setTimeout(function () {
                link.rel = 'stylesheet'
                link.media = 'only x'
            })
            // also enable media after 3 seconds,
            // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
            setTimeout(enableStylesheet, 3000)
        }

        // loop through link elements in DOM
        rp.poly = function () {
            // double check this to prevent external calls from running
            if (rp.support()) {
                return
            }
            var links = w.document.getElementsByTagName('link')
            for (var i = 0; i < links.length; i++) {
                var link = links[i]
                // qualify links to those with rel=preload and as=style attrs
                if (
                    link.rel === 'preload' &&
                    link.getAttribute('as') === 'style' &&
                    !link.getAttribute('data-loadcss')
                ) {
                    // prevent rerunning on link
                    link.setAttribute('data-loadcss', true)
                    // bind listeners to toggle media back
                    rp.bindMediaToggle(link)
                }
            }
        }

        // if unsupported, run the polyfill
        if (!rp.support()) {
            // run once at least
            rp.poly()

            // rerun poly on an interval until onload
            var run = w.setInterval(rp.poly, 500)
            if (w.addEventListener) {
                w.addEventListener('load', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            } else if (w.attachEvent) {
                w.attachEvent('onload', function () {
                    rp.poly()
                    w.clearInterval(run)
                })
            }
        }

        // commonjs
        if (typeof exports !== 'undefined') {
            exports.loadCSS = loadCSS
        } else {
            w.loadCSS = loadCSS
        }
    })(typeof global !== 'undefined' ? global : this)
</script>

    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>

    <link rel="preload" href="/css/style.css?v=20211217" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="/css/dark.css?v=20211217" as="style">
    <link rel="stylesheet" href="/css/dark.css">
    <link rel="stylesheet" href="/css/mobile.css?v=20211217" media="(max-width: 960px)">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" as="script">
    <link rel="preload" href="/scripts/main.js?v=20211217" as="script">
    <link rel="preload" href="/scripts/dark.js?v=20211217" as="script">
    <link rel="preload" href="/font/Oswald-Regular.ttf" as="font" crossorigin>
    <link rel="preload" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" as="font" crossorigin>
    <!-- algolia -->
    
    <!-- 百度统计  -->
    
    <!-- Google tag (gtag.js) -->
    

<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ == undefined) {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js" />')
        }
    </script>
    
        <body class="post-body">
    
        <!-- header -->
        <header class="header header-mobile">
    <!-- top read progress line -->
    <div class="header-element">
        <div class="read-progress"></div>
    </div>
    <!-- sidebar menu button -->
    <div class="header-element">
        
            <div class="header-sidebar-menu header-sidebar-menu-rounded">
        
            
                <i class="fas fa-bars"></i>
            
        </div>
    </div>
    <!-- header actions -->
    <div class="header-actions">
        <!-- theme mode switch button -->
        <span class="header-theme-btn header-element">
            <i class="fas fa-adjust"></i>
        </span>
        <!-- back to home page text -->
        <span class="home-link header-element">
            <a href=/>TUNIVERSE</a>
        </span>
    </div>
    <!-- toggle banner for post layout -->
    
        
            <div class="banner">
        
            <div class="blog-title header-element">
                <a href="/">TUNIVERSE</a>
            </div>
            <div class="post-title header-element">
                <a href="#" class="post-name">automl-paper-survey</a>
            </div>
        </div>
    
</header>

        <!-- fixed footer -->
        <footer class="footer-fixed">
    <!-- back to top button -->
    <div class="footer-fixed-element">
        
            <div class="back-top back-top-hidden back-top-rounded">
        
        
            <i class="fas fa-chevron-up"></i>
        
        </div>
    </div>
</footer>

        <!-- wrapper -->
        <div class="wrapper">
            <div class="site-intro" style="







    height:50vh;

">
    
    <!-- 主页  -->
    
        
    <!-- 404页  -->
    
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/intro/lsrf.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
                automl-paper-survey
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
                
            <!-- 404 -->
            
        </p>
        <!-- 文章页 meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class= post-intro-tags >
    
    
</div>

                
                <!-- 文章字数统计 -->
                
                    <div class="post-intro-read">
                        <span>字数统计: <span class="post-count word-count">7.1k</span>阅读时长: <span class="post-count reading-time">39 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <!-- 撰写日期 -->
                    <span class="iconfont-archer post-intro-calander">&#xe676;</span>
                    <span class="post-intro-time">2024/06/20</span>
                    <!-- busuanzi -->
                    
                        <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                            <span class="iconfont-archer post-intro-busuanzi">&#xe602;</span>
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    
                    <!-- 文章分享 -->
                    <span class="share-wrapper">
                        <span class="iconfont-archer share-icon">&#xe71d;</span>
                        <span class="share-text">Share</span>
                        <ul class="share-list">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>

            <script>
  // get user agent
  function getBrowserVersions() {
    var u = window.navigator.userAgent
    return {
      userAgent: u,
      trident: u.indexOf('Trident') > -1, //IE内核
      presto: u.indexOf('Presto') > -1, //opera内核
      webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
      gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
      mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
      ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
      android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
      iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
      iPad: u.indexOf('iPad') > -1, //是否为iPad
      webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
      weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
      uc: u.indexOf('UCBrowser') > -1, //是否为android下的UC浏览器
    }
  }
  var browser = {
    versions: getBrowserVersions(),
  }
  console.log('userAgent: ' + browser.versions.userAgent)

  // callback
  function fontLoaded() {
    console.log('font loaded')
    if (document.getElementsByClassName('site-intro-meta')) {
      document
        .getElementsByClassName('intro-title')[0]
        .classList.add('intro-fade-in')
      document
        .getElementsByClassName('intro-subtitle')[0]
        .classList.add('intro-fade-in')
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in')
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb() {
    if (browser.versions.uc) {
      console.log('UCBrowser')
      fontLoaded()
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular'],
        },
        loading: function () {
          // 所有字体开始加载
          // console.log('font loading');
        },
        active: function () {
          // 所有字体已渲染
          fontLoaded()
        },
        inactive: function () {
          // 字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout')
          fontLoaded()
        },
        timeout: 5000, // Set the timeout to two seconds
      })
    }
  }

  function asyncErr() {
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0]
    o.src = u
    if (cb) {
      o.addEventListener(
        'load',
        function (e) {
          cb(null, e)
        },
        false
      )
    }
    if (err) {
      o.addEventListener(
        'error',
        function (e) {
          err(null, e)
        },
        false
      )
    }
    s.parentNode.insertBefore(o, s)
  }

  var asyncLoadWithFallBack = function (arr, success, reject) {
    var currReject = function () {
      reject()
      arr.shift()
      if (arr.length) async(arr[0], success, currReject)
    }

    async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack(
    [
      'https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js',
      'https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js',
      "/lib/webfontloader.min.js",
    ],
    asyncCb,
    asyncErr
  )
</script>

            <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
            <div class="container container-unloaded">
                <main class="main post-page">
    <article class="article-entry">
        <p>。</p>
<span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="machine-learning"><a href="#machine-learning" class="headerlink" title="machine learning"></a>machine learning</h2><ul>
<li>the <strong>learning algorithm</strong> outputs a model whereas <strong>the model</strong> outputs predictions<ul>
<li>the learning algorithm  take in pre-processed data and output a model</li>
<li>then the model is different in the sense that it takes in new inputs or just inputs and its outputs are predictions for those inputs</li>
</ul>
</li>
<li>difference between <strong>parameter</strong> and <strong>hyperparameter</strong><ul>
<li>the <strong>model</strong> is often associated with <strong>parameters</strong> denoted by the symbol and parameters <strong>control the behavior of the model</strong> which is <strong>determined by the learning algorithm</strong>, so the learning algorithm often outputs these parameters and we do not have to think about them ourselves as human experts</li>
<li>the <strong>learning algorithm also has parameters</strong> but to distinguish these from the model parameters we call them <strong>hyperparameters</strong>. It <strong>control the learning outward</strong> and these are often <strong>determined by the humans</strong>. </li>
<li>example:<ul>
<li><strong>parameters</strong>: the thresholds in every node and maybe the rows that we use to go either left or right (for a neural network and in a decision tree)</li>
<li><strong>hyperparameter</strong>: something like the learning rate, the momentum value and so on (when trained with gradient descent)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="role-of-the-human-experts"><a href="#role-of-the-human-experts" class="headerlink" title="role of the human experts"></a>role of the human experts</h2><ul>
<li>what <strong>data processing</strong> steps: cleaning, feature engineering like what feature extraction and deed into the learning algorithm etc</li>
<li>what <strong>learning algorithm</strong> do we use, svm, logistic regression, neural network</li>
<li>what hyperparameters do we use for the given learning algorithm (<strong>hyperparameters selection</strong>)</li>
</ul>
<p><strong>automated machine learning &#x3D; automating one or more of these things</strong></p>
<h2 id="ml-x3D-gt-automl"><a href="#ml-x3D-gt-automl" class="headerlink" title="ml &#x3D;&gt; automl"></a>ml &#x3D;&gt; automl</h2><p>when determine data processing, learning algorithm and hyperparameters:</p>
<p>we introduce <strong>an automl agent</strong> which is either responsible for part of the choices that have to be made or it can fully replace the human and be responsible for making all of these choices.</p>
<p><img src="/2024/06/20/automl-paper-survey/image-20240826125411325.png" alt="image-20240826125411325"></p>
<h2 id="why-automate-ml"><a href="#why-automate-ml" class="headerlink" title="why automate ml"></a>why automate ml</h2><p>choice of preprocessing, learning algorithm and hyperparameters is <strong>crucial</strong> for achieving  good performance</p>
<ul>
<li>finding good setting for these components <strong>requires expert domain knowledge</strong> so this is something often done by data scientists who understand the machine learning algorithm and understand the influence of the different choices on the performance</li>
<li><strong>takes a lot of tedious effort and is expensive</strong> to actually find correct settings for all of these components in order to be satisfied with the performance</li>
<li>having a human finding good settings for these components maybe <strong>suboptimal</strong></li>
</ul>
<p>use automl we can </p>
<ul>
<li><p>no longer need expert domain knowledge</p>
</li>
<li><p><strong>increase the wide applicability and lower the threshold for actually using these techniques</strong> </p>
</li>
<li><p>better performance</p>
</li>
</ul>
<h2 id="auto-ml"><a href="#auto-ml" class="headerlink" title="auto ml"></a>auto ml</h2><p>consists of 3 parts:</p>
<ul>
<li><strong>search space</strong>: all of the different hyperparameters values maybe pre-processing steps etc over which the agent can search</li>
<li><strong>search algorithm</strong>: determines how it is going to search through this search spase </li>
<li><strong>evaluation mechanism</strong>: what we do in search i s we want to try to maximize the performance, so we need to have some kinds of evaluation mechanism <ul>
<li>that allows us to <strong>distinguish candidate solutions</strong> in the search space from each other </li>
<li>and which allows us to <strong>guide the search</strong> towards more and more promising or well-performing methods</li>
</ul>
</li>
</ul>
<h2 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h2><p>overview$^{[1]}$, we only care about the part of neural architecture search (NAS) </p>
<p><img src="/2024/06/20/automl-paper-survey/image-20240627163937730.png" alt="image-20240627163937730"></p>
<ul>
<li>search space</li>
<li>architecture optimization<ul>
<li>evolutionary algorithm</li>
<li>reinforcement learning</li>
<li>gradient descent</li>
<li>bandit-based<ul>
<li>successive halving</li>
<li>hyperba</li>
</ul>
</li>
<li><strong>surrogate model-based optimization</strong><ul>
<li>EGO&#x2F;TB-SPO</li>
<li><strong>bayesian optimization (BO)</strong><ul>
<li>Gaussian process (GP)</li>
<li>random forest (RF)</li>
<li>tree-structured parzen estimator (TPE)</li>
<li>bo-based hyperband (BOHB), combines the strengths of TPE-based BO and hyperband</li>
</ul>
</li>
<li>neural networks</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="CASH-vs-HPO"><a href="#CASH-vs-HPO" class="headerlink" title="CASH vs HPO"></a>CASH vs HPO</h1><h2 id="definition-of-CASH"><a href="#definition-of-CASH" class="headerlink" title="definition of CASH"></a>definition of CASH</h2><p>CASH &#x3D; combined algorithm selection and hyperparameter optimization</p>
<p>given:</p>
<ul>
<li>$\mathcal{T}$: task we want to solve</li>
<li>$\mathcal{D}_{train}$: the training data</li>
<li>$\mathcal{L_T}$: the loss function &#x3D;&gt; performance metric which measures how poorly our solutions are performing</li>
<li>(we want to find the best algorithm and the best hyperparameters)<ul>
<li>$\mathcal{A}$: the space of all algorithms over which we search</li>
<li>$\Theta$: the space of hyperparameters over which we search</li>
</ul>
</li>
</ul>
<p>we want to find:</p>
<p>$$A^*, \theta^* &#x3D; \arg\min\limits_{A \in \mathcal{A}, \theta \in \Theta} \space \mathbb{E}<em>\mathcal{pT} [\mathcal{L_T}(A_\theta(D</em>{train}))]$$</p>
<p>where</p>
<ul>
<li><p>hyperparameter vector $\theta &#x3D; [\theta_1 \theta_2 \dots \theta_n]$</p>
<ul>
<li>e.g. , [learning rate, momentum, use_batch_norm]</li>
</ul>
</li>
<li><p>$A_\theta(D_{train})$ means the <strong>output of learning algorithm</strong>, namely the <strong>model</strong> (the algorithm takes in the data and outputs a model)</p>
</li>
<li><p>$\mathcal{L_T}(A_\theta(D_{train}))$​ means the <strong>loss of the model</strong> on a given data point </p>
</li>
<li><p>$\mathbb{E}_\mathcal{pT}$​ means <strong>the expected or mean loss</strong> of the model</p>
</li>
</ul>
<p>in short, we want to find the <strong>best of algorithm and hyperparameter</strong> such that we minimize…</p>
<h2 id="definition-of-HPO"><a href="#definition-of-HPO" class="headerlink" title="definition of HPO"></a>definition of HPO</h2><p>based on definition of CASH, assume that the algorithm is given so we no longer want to find a best algorithm</p>
<p><strong>given</strong> a certain learning algorithm, we only want to <strong>find</strong> the best hyperparameters:</p>
<p>$$\theta^* &#x3D; \arg\min\limits_{\theta \in \Theta} \space \mathbb{E}<em>\mathcal{pT} [\mathcal{L_T}(A_\theta(D</em>{train}))]$$</p>
<h2 id="relationship"><a href="#relationship" class="headerlink" title="relationship"></a>relationship</h2><p>HPO &#x3D; CASH when <strong>including the algorithm as “hyperparameters”</strong></p>
<p>&#x3D;&gt; <u>将【算法】作为超参数包含在我们的结果$\theta$中</u></p>
<h1 id="GS-vs-RS"><a href="#GS-vs-RS" class="headerlink" title="GS vs RS"></a>GS vs RS</h1><h2 id="grid-search"><a href="#grid-search" class="headerlink" title="grid search"></a>grid search</h2><h3 id="definition"><a href="#definition" class="headerlink" title="definition"></a>definition</h3><ul>
<li>define sets of values you want to try for some (or all)  hyperparameters, evaluate <strong>all possible combinations</strong> &#x3D;&gt; traversal</li>
</ul>
<p><strong>value sets</strong>:</p>
<p>$v(\theta_1) &#x3D; { 0.1, 0.01, 0.001 }$</p>
<p>$v(\theta_2) &#x3D; { 0.99, 9.9,  0.8 }$</p>
<p>$\dots$</p>
<p>$v(\theta_n) &#x3D; { True, False }$</p>
<p><strong>evaluate</strong>:</p>
<p>$v(\theta_1) \times v(\theta_2) \times \dots \times v(\theta_n)$</p>
<p>this evaluation procedure is often done on a validation set, not training set because that will give us a biased view and may lead to over-fitting</p>
<h3 id="pros-and-cons"><a href="#pros-and-cons" class="headerlink" title="pros and cons"></a>pros and cons</h3><ul>
<li>pro<ul>
<li>simple</li>
<li>easy to parallelize</li>
</ul>
</li>
<li>cons<ul>
<li><strong>waste</strong> of compute when one or more hpms are not important</li>
<li><strong>scalability</strong> (exponential growth number of combinations)</li>
<li>how to define the grid: how do we know that <strong>the values that we put in the value sets are actually good values to try out</strong><ul>
<li>this is what <strong>random search</strong> actually kind of solves</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="random-search"><a href="#random-search" class="headerlink" title="random search"></a>random search</h2><h3 id="definition-1"><a href="#definition-1" class="headerlink" title="definition"></a>definition</h3><ul>
<li><p>define <strong>probability distribution</strong>s on <strong>ranges</strong> of values for every hyperparameters, draw hyperparameters configurations and evaluate</p>
</li>
<li><p><strong>sample from the range and sample according to some kind of probability distribution that we have defined over this range</strong></p>
</li>
</ul>
<p><strong>distributions</strong>:</p>
<p>$p(\theta_1) &#x3D; LogUniform(0.001, 0.1)$ &#x3D;&gt; we transform both of these two values (minimum and maximum) into log space, we sample in the log space and then we transform the sampled values back to the original space</p>
<p>$p(\theta_2) &#x3D; Uniform(0.8, 0.99)$</p>
<p>$\dots$</p>
<p>$p(\theta_n) &#x3D; Categorical({ True, False })$</p>
<p> <strong>evaluate M configurations</strong>:</p>
<p>$\theta \sim p(\theta_1, \theta_2, \dots, \theta_n)$</p>
<p>we evaluate samples from <strong>the joint distribution</strong></p>
<p>in order to <strong>sample one configuration</strong>, we have to sample the first value from the first distribution, the second value from the second distribution etc</p>
<p>so we need to sample <strong>in total from M times</strong> in n distributions</p>
<h3 id="pros-and-cons-1"><a href="#pros-and-cons-1" class="headerlink" title="pros and cons"></a>pros and cons</h3><ul>
<li>pros<ul>
<li>simple</li>
<li>easy to parallelize</li>
<li>can set a budget of function evaluations (evaluate M hyperparameter configurations)</li>
<li>theoretical guarantees of convergence with proper ranges<ul>
<li>not really has any meaning in practical scenarios</li>
</ul>
</li>
</ul>
</li>
<li>cons<ul>
<li>not data efficient</li>
<li>still expensive</li>
</ul>
</li>
</ul>
<h2 id="relationship-1"><a href="#relationship-1" class="headerlink" title="relationship"></a>relationship</h2><p><img src="/2024/06/20/automl-paper-survey/image-20240826151400164.png" alt="image-20240826151400164"></p>
<p>random search is more efficient when there are unimportant hyperparameters (better coverage)</p>
<h1 id="Successive-Halving-vs-Hyperband"><a href="#Successive-Halving-vs-Hyperband" class="headerlink" title="Successive Halving vs Hyperband"></a>Successive Halving vs Hyperband</h1><p>downside of gs and rs: every hyperparameters configuration is <strong>fully trained</strong> &#x3D;&gt; expensive</p>
<p>we can detect early on that a configuration will be bad and discard it, so techniques to do this:</p>
<ul>
<li>successive halving</li>
<li>hyperband &#x3D;&gt; the extension of successive halving</li>
</ul>
<h2 id="successive-halving"><a href="#successive-halving" class="headerlink" title="successive halving"></a>successive halving</h2><h3 id="idea"><a href="#idea" class="headerlink" title="idea"></a>idea</h3><p>on the <strong>x-axis</strong> we have compute budget which can think of as the total number of epochs</p>
<p>on the <strong>y-axis</strong> we have the performance metric, here is loss</p>
<p><img src="/2024/06/20/automl-paper-survey/image-20240826155923846.png" alt="image-20240826155923846"></p>
<p>in round 0 we start a randomly sampled configuration.</p>
<p>(after making a number of training steps we evaluate all configurations in this round) &#x3D;&gt; <strong>throw the worst half off and continue training the best half</strong></p>
<ul>
<li>we simply throw away the worst half of these configurations</li>
<li>we stop training them and we continue training the best half</li>
</ul>
<p><img src="/2024/06/20/automl-paper-survey/image-20240826160706655.png" alt="image-20240826160706655"></p>
<h3 id="definition-2"><a href="#definition-2" class="headerlink" title="definition"></a>definition</h3><ul>
<li><p>start a set of $n$ random initial configurations ${ A_\theta^{(i)} }^n_{i&#x3D;1}$ and a budget $B$</p>
</li>
<li><p>every round the candidate pool size is divided by $\gamma$​ which means halving rate</p>
</li>
<li><p>we only perform $log_\gamma(n)$​ rounds</p>
</li>
<li><p>budget is <strong>divided uniformly rounds</strong></p>
<ul>
<li>but because we throw away configurations in every round  what this basically means is that <strong>every configuration is assigned more budget per round</strong> </li>
<li>consequently we <strong>assign exponentially more budget to good configurations</strong></li>
</ul>
</li>
<li><p>example</p>
<ul>
<li>number of initial configurations n &#x3D; 64. total budget B &#x3D; 384, halving rate $\gamma$ &#x3D; 2 </li>
<li>computation metric<ul>
<li>round &#x3D;&gt; k</li>
<li>number of candidate pool in this round &#x3D;&gt; $S_k$</li>
<li>budget in this round per configuration &#x3D;&gt; $r_k$​</li>
</ul>
</li>
<li>procedure<ul>
<li>$log_2(64) &#x3D; 6$, so we only need to perform 6 rounds, namely k &#x3D; { 0, 1, 2, 3, 4, 5 }</li>
<li>$\gamma &#x3D; 2$, so $S_k$​ &#x3D; { 64, 32, 16, 8, 4, 2 }</li>
<li>since budget is divided uniformly rounds, we have 6 rounds here, the budget of each round is 384&#x2F;6 &#x3D; 64. then we divide the total budget per round by $S_k$, namely 64&#x2F;$S_k$, so $r_k$ &#x3D; {1, 2, 4, 8, 16, 32 }</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="hyperparameters"><a href="#hyperparameters" class="headerlink" title="hyperparameters"></a>hyperparameters</h3><ul>
<li><p>higher budget &#x3D;&gt; higher budget per configuration (<strong>more informed decisions and more compute time</strong>s)</p>
</li>
<li><p>higher halving rate &#x3D;&gt; more aggressive pruning (could be too aggressive and <strong>disregard the best candidate early o</strong>n, imagine a configuration which starts learning quite slow and only after a certain number of epochs starts to really maximize the performance)</p>
</li>
</ul>
<h3 id="pros-and-cons-2"><a href="#pros-and-cons-2" class="headerlink" title="pros and cons"></a>pros and cons</h3><ul>
<li><p>pros</p>
<ul>
<li>allow us to work with fixed budget</li>
<li>good experical results</li>
<li>strong theoretical foundations</li>
<li>parallelizable</li>
</ul>
</li>
<li><p>cons</p>
<ul>
<li><p>learning curves can cross (good candidates to be discarded early)</p>
</li>
<li><p>n &#x2F; B trade-off, given a fixed budget B </p>
<p><img src="/2024/06/20/automl-paper-survey/image-20240826170638588.png" alt="image-20240826170638588"></p>
<ul>
<li>larger n &#x3D;&gt; smaller amount of time per config</li>
<li>smaller n &#x3D;&gt; larger amount of time per config</li>
<li>we do not know which works best &#x3D;&gt; <strong>this is what hyperband actually solves</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="hyperband"><a href="#hyperband" class="headerlink" title="hyperband"></a>hyperband</h2><h3 id="idea-1"><a href="#idea-1" class="headerlink" title="idea"></a>idea</h3><p>running successive halving <strong>different times with different values of n</strong></p>
<ul>
<li>firstly, it does <strong>a bracket of successive halving</strong>  using a large number of initial configurations </li>
<li>then it simply randomly sample <strong>another set of initial configurations</strong> and this time <strong>lower the amount</strong> than in our previous round </li>
<li>we again decrease the candidate pool size until we are in our final bracket which is equivalent to a random search</li>
</ul>
<h3 id="definition-3"><a href="#definition-3" class="headerlink" title="definition"></a>definition</h3><p>perform different successive halving with hyperband (here we call bracket) $s_{max} + 1$ times with various initial candidate pool size $n$ (in essence performing a grid search over feasible value of $n$) for a fixed budget $B$</p>
<ul>
<li><strong>start with large $n$</strong> (we do quite some exploration, but less time per configuration), subject to the constraint that <strong>at least one configuration is allocated R resources.</strong> </li>
<li>every consecutive bracket, <strong>reduce n</strong> (we decrease the n which means that we will get more time per configuration but a smaller number of configurations and this also leads to less exploration) <strong>until the final bracket</strong>, s &#x3D; 0, in which <strong>every configuration is allocated R resources</strong> (this bracket simply performs classical <strong>random search</strong>)</li>
</ul>
<p>conclusion &#x3D;&gt; there are two components to hyperband</p>
<ul>
<li><strong>the inner loop</strong> invokes successive halving for fixed values of n and r</li>
<li><strong>the outer loop</strong> iterates over different values of n and r</li>
</ul>
<h3 id="hyperparameters-1"><a href="#hyperparameters-1" class="headerlink" title="hyperparameters"></a>hyperparameters</h3><ul>
<li><p>R: maximum resource that can be allocated to a single configuration within a round of successive halving</p>
</li>
<li><p>$\gamma$: control proportion or discarded configurations every round</p>
</li>
<li><p>example</p>
<ul>
<li>various brackets, with R &#x3D; 81 and $\gamma &#x3D; 3$​</li>
<li>firstly, we consider the number of different brackets we need to perform, it’s $s_{max} + 1 &#x3D; log_{\gamma}(R) + 1 &#x3D; log_3(81) + 1 &#x3D; 4 + 1 &#x3D; 5$. and  $B &#x3D; (s_{max} + 1) R &#x3D; 5R$</li>
<li><strong>In the $s_{max}$ bracket</strong>: since we have a fixed budget R &#x3D; 81, according to the algorithm, we start with a large number of configurations n, so here n is equivalent to R, namely 81 (or we can compute according to the equation $n &#x3D; \gamma^{s_{max}} &#x3D; 3^4 &#x3D; 81$). so in the round zero, budget for each configuration $r_i$ is R&#x2F;n &#x3D; 81&#x2F;81 &#x3D; 1. then in the next round of this bracket we perform the algorithm as successive halving until the budget for each configuration in the round is equal to R</li>
<li><strong>then in the third bracket</strong>, we decrease the initial number of configurations using given halving rate $\gamma &#x3D; 3$, so <strong>n of  round zero of second bracket</strong> should be $81 &#x2F; 3 &#x3D; 27$ (<strong>we can compute according to the equation $n &#x3D; \lceil  \frac{5R}{R} \frac{\gamma^s}{s+1} \rceil &#x3D; \lceil 5 \frac{3^3}{3+1} \rceil &#x3D; 34$, so we chose 27 because 27 &lt; 34 and the output of $log_3(27)$ is an integer</strong>). note that we have fixed maximum budget for all configurations in each round, still be R &#x3D; 81, so this time the budget for each configuration $r_0 &#x3D; R&#x2F;n_0 &#x3D; 81&#x2F;27 &#x3D; 3$</li>
<li><strong>in the second bracket</strong>, we perform as before. the initial value of $n_i$ and $r_i$ should be 27&#x2F;3&#x3D;9 and 81&#x2F;9&#x3D;9 ($n &#x3D; \lceil  \frac{5R}{R} \frac{\gamma^s}{s+1} \rceil &#x3D; \lceil 5 \frac{3^2}{2+1} \rceil &#x3D; 15$, so we chose 9)</li>
</ul>
<p><img src="/2024/06/20/automl-paper-survey/image-20240826223328862.png" alt="image-20240826223328862"></p>
<p><img src="/2024/06/20/automl-paper-survey/image-20240826203654749.png" alt="image-20240826203654749"></p>
</li>
</ul>
<h1 id="SMBO"><a href="#SMBO" class="headerlink" title="SMBO"></a>SMBO</h1><p>surrogate model-based optimization</p>
<h2 id="limitation-of-techinique-so-far"><a href="#limitation-of-techinique-so-far" class="headerlink" title="limitation of techinique so far"></a>limitation of techinique so far</h2><ul>
<li>they <strong>randomly select initial candidates</strong> to evaluate</li>
<li>but if we have already evaluated a set of candidates, we can do better than random selection<ul>
<li>predict promising regions <strong>based on history of observations</strong></li>
<li>we could <strong>fit a model to our previous observations</strong> and then use this model to predict the most promising regions or <strong>the most promising hyperparameter configurations</strong></li>
</ul>
</li>
</ul>
<h2 id="idea-2"><a href="#idea-2" class="headerlink" title="idea"></a>idea</h2><ul>
<li><p>learn a <strong>surrogate model</strong> that for every configuration $\theta \in \Theta$:</p>
<ul>
<li><p>predicts the loss or <strong>performance</strong></p>
</li>
<li><p>provides a measure of <strong>uncertainty</strong> about the prediction</p>
</li>
</ul>
</li>
<li><p>loosely speaking, the surrogate models $P(y \vert \theta)$, where y denotes the performance and $\theta$​ denotes a given hyperparameter</p>
</li>
<li><p>so we need to <strong>update the surrogate model sequentially</strong> as new observations (tried configuration, performance) arrive</p>
</li>
<li><p>and we use the surrogate model to <strong>select new promising configurations to evaluate</strong></p>
</li>
</ul>
<p><img src="/2024/06/20/automl-paper-survey/image-20240627164911129.png" alt="image-20240627164911129"></p>
<ul>
<li>symbol explanations<ul>
<li>evaluation function &#x3D;&gt; $f$</li>
<li>search space &#x3D;&gt; $\Theta$</li>
<li>acquisition function &#x3D;&gt; $S$</li>
<li>probability model &#x3D;&gt; $M$</li>
<li>dataset $D$, it includes many sets of samples $(θ_i, y_i)$ where $\theta_i \in \Theta$ and $y_i$ denotes the performance of $\theta_i$​</li>
</ul>
</li>
</ul>
<h2 id="instantiation"><a href="#instantiation" class="headerlink" title="instantiation"></a>instantiation</h2><ul>
<li>bayesian optimization<ul>
<li>start with a prior distribution, namely start with initial surrogate model and as data points come in and as we try out more configuration and observe their performances</li>
<li>we repeat: update our beliefs about the loss surface so that we update our surrogate model</li>
</ul>
</li>
</ul>
<h2 id="surrogate-model"><a href="#surrogate-model" class="headerlink" title="surrogate model"></a>surrogate model</h2><ul>
<li><p>Gaussian process (good <strong>confidence bounds</strong>, however only <strong>numerical</strong> hyperparameters)</p>
<ul>
<li>face some <strong>challenge</strong> when having <strong>categorical</strong> hyperparameter, it’s not easy to incorporate it</li>
</ul>
</li>
<li><p><strong>random forest</strong> (categorical and conditional hyperparameters)</p>
<ul>
<li><strong>categorical</strong> &#x3D;&gt; a boolean flag where we want to use batch normalization in a neural network</li>
<li><strong>conditional</strong> &#x3D;&gt; hyperparameters that are only defined if some other hyperparameter is satisfied with some constraints. suppose we have a neural network and we have one hyperparameter that is the number of nodes in layer number 3, now for this hyperparameter to actually carry any meaning we need to have the other hyperparameter value which is the number of layers to be larger or equal than 3 because otherwise this one hyperparameter does not make any sense.</li>
</ul>
</li>
</ul>
<h1 id="BO"><a href="#BO" class="headerlink" title="BO"></a>BO</h1><blockquote>
<p>referencing the blog post$^{[2]}$, this part add some understanding and other supplementary content related to mathematics</p>
</blockquote>
<h2 id="似然估计与其函数"><a href="#似然估计与其函数" class="headerlink" title="似然估计与其函数"></a>似然估计与其函数</h2><p>回顾总结一下遗忘的知识：</p>
<ol>
<li>先验概率：在结果发生前就获得的、根据历史规律确定原因的概率，即 $p(因) &#x3D; p(x)$；</li>
<li>后验概率：已知结果，猜测原因，即 $p(因 \vert 果) &#x3D; p(x \vert y)$</li>
<li>似然估计：已知原因，猜测原因对应的结果的概率，即 $p(果\vert 因) &#x3D; p(y \vert x)$​</li>
</ol>
<p><strong>联合概率密度函数</strong>：$p(x, y) &#x3D; p(x \vert y)p(y) &#x3D; p(y \vert x)p(x)$</p>
<p><strong>贝叶斯公式</strong>：$p(x \vert y) &#x3D; \frac{p(y \vert x)p(x)}{p(y)} &#x3D; \frac{p(y \vert x)p(x)}{\int p(y \vert x)p(x) dx}$​，根据似然概率和先验概率求后验概率。</p>
<p>而似然函数是用于描述观测数据与某个概率模型之间的关系的函数。假设有个概率模型，由一组未知参数 $\theta$ 描述，有一组观测数据 $D_0$，那么似然函数 $L(\theta | D_0)$ 就表示在给定数据 $D_0$ （因）下，参数 $\theta$ （果）取某个值的概率。</p>
<p>$$L(\theta|D_0) &#x3D; P(D &#x3D; D_0|\theta)$$</p>
<p>$P(D&#x3D;D_0|θ)$ 表示在参数 $θ$ （果）的条件下，数据 $D_0$ （因）出现的概率，后验概率。</p>
<p>通过最大化似然函数，我们可以找到参数 $θ$ 的最优估计值，也即使观测数据出现概率最大的参数值。</p>
<h2 id="problem-description"><a href="#problem-description" class="headerlink" title="problem description"></a>problem description</h2><p>$x^{*} &#x3D; arg\space \max\limits _{x \in \mathcal{X}} f(x)$</p>
<ul>
<li>这里 $\mathcal{X}$ 是超参数空间，比如对于一个随机森林模型，这里 $\mathcal{X}$ 就是模型和模型对应的参数，即 {  random_forest, n_ estimators, max_depth }。</li>
<li>$f(x)$ 是参数空间到我们感兴趣的一个指标的映射函数，如回归问题中的MSE或者分类问题中的accuracy和AUC指标。注意不是数据集到预测结果的映射，而是**在某个数据集下，某个模型及其相关的参数$\mathcal{X}$到我们关心的指标的映射 **，如 $f(x): \mathcal{X} \rightarrow acc$。</li>
<li>可以认为$f(x)$是一个黑盒函数，其计算代价很高，解决该优化问题被称为黑箱优化。</li>
<li>注意$f(x)$的凹凸性质，没有一阶或高阶导数，不能通过梯度下降或者牛顿相关算法进行求解，但我们知道$f(x)$是<strong>连续函数</strong>，因此可以使用<strong>高斯回归过程对$f(x)$进行建模</strong>。</li>
</ul>
<h2 id="iterating-method"><a href="#iterating-method" class="headerlink" title="iterating method"></a>iterating method</h2><p>对于 $y&#x3D;f(x)$，假设有个先验概率，每次有新的数据 ${ x_{data}, f(x_{data}) }$，求出y的后验概率 $p(y | (x_{data}, f(x_{data})))$，并把这个后验概率作为下一条数据的先验概率，如此迭代更新。</p>
<p><strong>重点是</strong>我们需要一个规则来找到基于当前情况的下一个迭代数据 $x_{next}$。所以我们需要基于当前 $y$ 的后验概率，定义一个acquisition function： $a_n(x | data)$，$x \rightarrow R$，这个函数用于计算在当前情况下不同的数据 $x$ 对 $y$ 的作用，从而去选择新的下一条数据 $x_{next}$ 以最大化 $a_n$。</p>
<h2 id="algorithm-procedure"><a href="#algorithm-procedure" class="headerlink" title="algorithm procedure"></a>algorithm procedure</h2><ol>
<li>给定一个 $y&#x3D;f(x)$ 的先验概率 $p_0$，最大迭代次数 $N$</li>
<li>随机初始化 $n_0$ 个点，并得到 $n_0$ 个点结果，即 ${ (x_1, f(x_1)), (x_2, f(x_2), …, (x_{n_0}, f(x_{n_0}))) }$</li>
<li>利用初始化的$n_0$个点，更新先验概率，并更新 $n \leftarrow n_0$</li>
<li>当 $n \leq N$ 时：<ul>
<li>根据当前的后验概率 $p(y | { (x_1, f(x_1)), (x_2, f(x_2), …, (x_{n_0}, f(x_{n_0}))) })$ 来计算 $a_n (x)$</li>
<li>选择能够最大化 $a_n(x)$ 的点作为 $x_{n+1}$。<strong>注：这一步是重点，该如何找到下一个迭代点 $x_{next}$ 以及过程中如何对 $p(y \vert x)$ 和 $p(y|x_{next})$ 进行建模</strong></li>
<li>将新的点 $x_{n+1}$ 带入得到 $f(x_{n+1})$ ，会非常耗时</li>
<li>$x \leftarrow n + 1$</li>
</ul>
</li>
<li>返回评估过的数据中使得 $f(x)$ 最大的点 $x^{*}$​</li>
</ol>
<h1 id="GP"><a href="#GP" class="headerlink" title="GP"></a>GP</h1><p>Gaussian Process Regression</p>
<p>主要解决前面提到的贝叶斯推断中的两个重点问题：1）概率建模；2）下一个迭代数据点的确认。</p>
<h2 id="probability-modeling"><a href="#probability-modeling" class="headerlink" title="probability modeling"></a>probability modeling</h2><h3 id="多维正态分布的似然函数"><a href="#多维正态分布的似然函数" class="headerlink" title="多维正态分布的似然函数"></a>多维正态分布的似然函数</h3><p>对于贝叶斯优化的初始化数据 ${ (x_1, f(x_1)), (x_2, f(x_2), …, (x_{n_0}, f(x_{n_0}))) }$，假设多维似然函数 $L$ 服从多维正态分布，后验概率密度为：</p>
<p>$p(y) &#x3D; \left( \begin{matrix} y_1 \ y_2 \ … \ y_{n_0} \end{matrix} \right) \sim N\left( \left(\begin{matrix} u_0(x_1) \ u_0(x_2) \ … \ u_0(x_{n_0}) \end{matrix}\right), \Sigma_{n_0 * n_0}\right)$</p>
<p>$\Sigma_{n_0 * n_0} &#x3D; \left [ \begin{matrix} k_{11}&amp;\dots&amp;k_{1 n_0} \ \vdots &amp; \ddots &amp; \vdots \ k_{n_01} &amp; \dots  &amp; k_{n_0n_0} \end{matrix} \right]$</p>
<p>注意这里 $\Sigma$ 是大写的 $\sigma$，一般来说 $\Sigma$ 是一个 $n_0 \times n_0$ 的协方差矩阵，这个矩阵描述了 $n_0$ 个数据两两之间的相关性。其计算方法：</p>
<ul>
<li>对角线元素 $\Sigma_{ii} &#x3D; Cov(x_i, x_i) &#x3D; K(x_i, x_i)$；</li>
<li>非对角线元素 $\Sigma_{ij} &#x3D; Cov(x_i, x_j) &#x3D; K(x_i, x_j)$</li>
</ul>
<p>这里对协方差矩阵的建模采用了核函数  $K(·,·)$，不同的核函数又会有不同的计算公式，如：</p>
<ul>
<li>线性核函数：$K(x_i, x_j) &#x3D; x_i^T x_j$，对两个向量做内积</li>
<li>高斯核函数：$K(x_i, x_j) &#x3D; e^{-\frac{||xi - xj||^2}{2σ^2}}$。<ul>
<li>$\sigma$ 是核函数的带宽参数，控制着核函数的平滑程度，$\sigma$ 越小，相关性变化越剧烈。</li>
<li>公式反映了不同变量 $x_i$ 和 $x_j$ 之间的相关性，距离才用了 $L2$ 距离，距离越近（$||x_i - x_j||^2$ 越小），相关性越强（$\Sigma_{ij}$ 越大），由公式得对角线元素为1。</li>
<li>该核函数满足了对称性且保证了协方差矩阵为非负正定矩阵（是一个对称矩阵，并且对于其中任意非零向量 $x$，$x$​ 与该矩阵的乘积都会得到一个正值）。</li>
<li>有时会让指数乘上一个其它的参数。</li>
</ul>
</li>
</ul>
<p><strong>（重要）</strong>为什么可以用 $x_i$ 和 $x_j$ 来反映 $y_i$ 和 $y_j$ 的关系？因为上面说过 $f(x)$ 是连续的！ </p>
<p>而均值向量中的 $u_0$ 是先验信息，即对于一个 $x$​ 先验的均值，在一开始就确定（一般设置为0）。</p>
<p>如此就根据已有数据确定了一个高斯过程。</p>
<h3 id="process"><a href="#process" class="headerlink" title="process"></a>process</h3><p>新加入的数据与之前数据继续构成多维正态分布，这一步需要得到当前数据点下 $p(y | x)$ 的分布，假设新的数据点为 $(x^*, f(x^*))$，且根据联合高斯概率密度函数：</p>
<p>$p(y, y_*) &#x3D; N\left( \left( \begin{matrix} \mu_y \ \mu_{y_*} \end{matrix} \right), \left( \begin{matrix} K &amp; K_* \ K_*^T &amp; k_{**}  \end{matrix} \right) \right)$</p>
<p>带入展开则得到新的数据点与之前的数据点一起构成新表达式：</p>
<p>$p(y, y_*) &#x3D; \left( \begin{matrix} y_1 \ y_2 \ … \ y_{n_0} \ y_* \end{matrix} \right) \sim N \left( \left(\begin{matrix} u_0(x_1) \ u_0(x_2) \ … \ u_0(x_{n_0}) \ u_0(x*) \end{matrix}\right), \Sigma_{(n_0 + 1) * (n_0 + 1)} \right) \ \Sigma_{(n_0 + 1) * (n_0 + 1)} &#x3D; \left [ \begin{matrix} k_{11} &amp; \dots &amp; k_{1{n_0}} &amp; k_{1*} \ \vdots &amp; \ddots &amp; \vdots &amp; \vdots \ k_{n_01} &amp; \dots  &amp; k_{n_0n_0} &amp; k_{n_0*}\ k_{*1} &amp; \dots &amp; k_{*n_0} &amp; k_{**} \end{matrix} \right]$</p>
<p>这里的 $K$ 即为原来 $y$ 的协方差矩阵，而 $K_* &#x3D; \left[ \begin{matrix} k_{*1} \ \vdots \ k_{*n_0} \end{matrix} \right]$ </p>
<p>同理，这里 $u_0(x^*)$ 页数对于任意一个 $x^*$ 来说模型最开始的先验概率均值，当先验确定时，该均值也相应确定。</p>
<h3 id="final-modeling"><a href="#final-modeling" class="headerlink" title="final modeling"></a>final modeling</h3><p>接下来根据贝叶斯公式：</p>
<p>$p(y, y_*) &#x3D; p(y_* \vert y)p(y)$</p>
<p>以及前面的内容推导后得到似然概率密度为：</p>
<p>$$p(y_* \vert y) &#x3D; N(K_*^T K^{-1}(y-\mu_y), k_{**} - K_<em>^T K K_</em>)$$</p>
<p>统一写法后即：</p>
<p>$p(y_* \vert x^*) \sim N\left({K_*^T} {\Sigma_{n_0 * n_0}}^{-1} (y_{n_0} - u_{n_0}), k_{**} - K_<em>^T \Sigma_{n_0 * n_0} K_</em> \right)\ y_{n_0} &#x3D; \left( \begin{matrix} y_1 \ y_2 \ … \ y_{n_0} \end{matrix} \right) \ u_{n_0} &#x3D; \left( \begin{matrix} u_0(x_1) \ u_0(x_2) \ … \ u_0(x_{n_0}) \end{matrix} \right)$</p>
<p>那么基于当前数据，有先验概率（即上次计算得到的后验概率 $p(y)$），对于每个新的点 $(x^*, y_*)$，都可以通过计算联合高斯密度函数计算出 $p(y, y_*)$，从而计算出 $p(y_* \vert x^*)$</p>
<h2 id="next-observation"><a href="#next-observation" class="headerlink" title="next observation"></a>next observation</h2><p>当解决了在确认数据下对 $y$ 的概率分布建模后，接下来的关键就是如何确定下一个迭代点 $x_{next}$，前面提到过是通过最大化 $a_n(·)$ 函数来确认。</p>
<p>关于 $a_n(·)$ 的定义有很多$^{[5]}$：</p>
<ul>
<li>Probability of Improvement</li>
<li><strong>Expected Improvement (EI)</strong> $^{[6]}$</li>
<li>Minimizing the Conditional Entropy of the Minimizer $^{[7]}$</li>
<li>the bandit-based criterion $^{[8]}$</li>
</ul>
<h3 id="EI"><a href="#EI" class="headerlink" title="EI"></a>EI</h3><p>当前 $y$ 的概率分布已知了，最简单的想法就是选择一个 $x^*$， 使得 $p(y_* \vert x^*)$ 的期望值最大，但这并不一定是一个最好的结果，因为当前算出来的新的后验概率不一定准确，基于均值最大不一定是最好的结果，我们期望得到一个全局最优解。</p>
<p>因此不去最大化后验均值，而是最大化（后验均值 + 一倍标准差）， 其中加标准差的目的是为了增加可探索性，因为标准差越大，则代表该点的y值会有更大概率取得一个较大的值（但同理也会取得一个更小的值），所以acquisition function 可以理解为一个效用函数，权衡当前已有的结果以及探索的可能。</p>
<h2 id="figure-example"><a href="#figure-example" class="headerlink" title="figure example"></a>figure example</h2><p>figure example on bayesian optimization$^{[7]}$​</p>
<ul>
<li><p>suppose</p>
<ul>
<li><p>we are trying to optimize a single hyperparameter which we have displayed on the x-axis.</p>
</li>
<li><p>and also we suppose that there is an objective function that we want to minimize and this is displayed on the y-axis </p>
</li>
<li><p>the dots denote what we have observed before</p>
</li>
</ul>
</li>
<li><p>what we do in bayesian optimization is we fit a surrogate model to these observations which is going to predict the objective function value for the hyperparameter value x</p>
</li>
<li><p>the curve here is the mean and the shaded areas display uncertainty about given points</p>
<ul>
<li>at point at which we have directly evaluated or observed the real or underlying objective function, the uncertainty is zero </li>
<li>so we can see in the point, the uncertainty is very low. as we move further and further away from observation, the uncertainty around the predictions actually increase which is logical because we have not explored points in that area.</li>
</ul>
</li>
<li><p>once we have the model, we can compute acquisition function which tell us how promising different areas of this hyperparameter space are</p>
<ul>
<li>once we have computed this we can actually select new points accordingly</li>
<li>as the uncertainty increases, the acquisition function also tends to increase a bit</li>
</ul>
</li>
</ul>
<p><img src="/2024/06/20/automl-paper-survey/image-20240627162942070.png" alt="image-20240627162942070"></p>
<h2 id="pros"><a href="#pros" class="headerlink" title="pros"></a>pros</h2><ul>
<li><strong>scalability</strong> &#x3D;&gt; do not typically scale well to high dimensions and exhibit cubic complexity in the number of data points </li>
<li><strong>flexibility</strong> &#x3D;&gt; do not apply to complex configurations spaces without special kernels</li>
<li><strong>robustness</strong> &#x3D;&gt; require carefully-set hyper-priors</li>
</ul>
<h1 id="SMAC"><a href="#SMAC" class="headerlink" title="SMAC"></a>SMAC</h1><p>as we mentioned before, if we use Gaussian process as the surrogate model, it can only support numerical hyperparameters. as for other SMBO algorithm, they all have <strong>3 key limitations</strong></p>
<ul>
<li>only support <strong>numerical hyperparameters</strong>, which is the same as GP</li>
<li>only optimizes target algorithm performance <strong>for single instances</strong></li>
<li>lacks a mechanism for <strong>terminating poorly performing target algorithm runs</strong> early (except successive halving and its extension hyperband)</li>
</ul>
<p>in order to address first two limitation, SMAC thus make SMBO applicable to general algorithm configuration problems with <strong>many categorical parameters</strong> and sets of benchmark instances</p>
<h2 id="idea-3"><a href="#idea-3" class="headerlink" title="idea"></a>idea</h2><blockquote>
<p>suppose there are some sampled configurations with their performance under fixed dataset, ${ (x_1, f(x_1)), (x_2, f(x_2)), \dots, (x_n, f(x_n)) }$. </p>
</blockquote>
<p>what <strong>SMAC</strong> do is do fit a model $f$​ <strong>based on random forest</strong>, it’s <u>likened to a multidimensional normal distribution formed by n observations in the Gaussian process</u>.</p>
<p>since random forests can handle discrete variables, this method naturally lends itself to cases where the variables are <strong>discrete</strong> (categorical). for scenarios involving <strong>conditional constraints</strong>, it is only necessary to incorporate these constraints into the parameter space to <strong>ensure that certain impossible situations are not sampled</strong>.</p>
<h2 id="modeling-process"><a href="#modeling-process" class="headerlink" title="modeling process"></a>modeling process</h2><p>in the Gaussian regression process, for a newly added point x forming a new multidimensional normal distribution with previous points, <strong>solving the marginal distribution</strong> of the newly added point yields its <strong>mean and standard deviation.</strong> </p>
<p>how is this achieved in the random forest model is that for a new point x, there is <strong>a prediction result on each tree</strong> in the random forest. <strong>averaging</strong> the prediction results of all trees <strong>yields the mean</strong>, and calculating the standard deviation of the prediction results gives the standard deviation. </p>
<p>another advantage of random forests is that for a point x, the prediction <strong>complexity</strong> on each tree is only <strong>$O(logN)$</strong>, whereas in the Gaussian regression process, complex matrix multiplication is required to obtain the mean and standard deviation.</p>
<h2 id="next-observation-1"><a href="#next-observation-1" class="headerlink" title="next observation"></a>next observation</h2><ul>
<li>select the <strong>top 10</strong> points in the current data set that maximize the acquisition function as starting points</li>
<li><strong>find neighboring points using the random neighbors method</strong>, and make predictions using the random forest model.<ul>
<li>random neighbors for a specific point &#x3D;&gt; </li>
<li>randomly switching the values of all its <strong>discrete variables</strong></li>
<li>sampling the <strong>continuous variable</strong>s with a mean equal to the current value and a standard deviation of 0.2 (continuous variables were normalized to [0,1] beforehand)</li>
</ul>
</li>
<li>by randomly sampling 10,000 points using this method, predicting their <strong>mean and standard deviation</strong> with the random forest</li>
<li>calculating the acquisition function, and selecting the point with the highest value as the next iteration point</li>
</ul>
<h1 id="TPE"><a href="#TPE" class="headerlink" title="TPE"></a>TPE</h1><p>tree parzen estimator</p>
<h2 id="definition-4"><a href="#definition-4" class="headerlink" title="definition"></a>definition</h2><p>instead of modeling the <strong>performance of given hyperparameter value</strong> $p(y \vert x)$, what we do in TPE is that we <strong>model the opposite</strong></p>
<ul>
<li>model the probability of observing a given hyperparameter value x given a certain loss value (or performance)</li>
<li>$p(x \vert y)$</li>
<li>x &#x3D; value of <strong>single</strong> hyperparameter</li>
<li>y &#x3D; loss (performance)</li>
</ul>
<p>maintain <strong>two surrogate models</strong></p>
<ul>
<li>a distribution for <strong>bad</strong> values &#x3D;&gt;  $p(x \vert y &gt; y^*) &#x3D; p(x \vert bad)$​<ul>
<li>$p(x \vert y &gt; y^*)$ means the probability distribution over the hyperparameter values given the loss y was worse than the threshold $y^*$ that we have determined</li>
<li>also can write as the probability of observing  hyperparameter value when given that the performance is bad</li>
</ul>
</li>
<li>a distribution for <strong>good</strong> values &#x3D;&gt; $p(x \vert y \leq y^*) &#x3D; p(x \vert good)$</li>
<li>here the threshold <strong>determines good&#x2F;bad</strong> split and it’s <strong>determined by hyperparameter $\gamma$​</strong><ul>
<li>suppose have 100 different observations and the gamma is set to 0.5, this means that we take the best 50 observations in order to fit the good distribution and we use the 50 worse to fit the bad distribution</li>
</ul>
</li>
</ul>
<p>get <strong>new promising candidates</strong> &#x3D;&gt; a promising candidate is likely to</p>
<ul>
<li>have <strong>low</strong> probability under the bad distribution $p(x \vert bad)$</li>
<li>have <strong>high</strong> probability under the good distribution $p(x \vert good)$</li>
<li><strong>promisingness</strong> $\varpropto \frac{p(x \vert good)}{p(x \vert bad)}$<ul>
<li>the promising score of a hyperparameter value x can be computed as the <strong>ratio between these two</strong> probabilities</li>
<li>according to the equation, promisingness score increases when the probability under the bad distribution is low or the probability under the good distribution is high</li>
<li>this result is actually <strong>proportional to the expected improvement (EI)</strong></li>
<li>the author use this promisingness to <strong>take place of normal EI as a new acquisition function</strong></li>
</ul>
</li>
</ul>
<h2 id="example"><a href="#example" class="headerlink" title="example"></a>example</h2><blockquote>
<p>the blue plot denotes the objective function we want to minimize and there are some points we have observed for the good group and bad group accordingly. </p>
</blockquote>
<p>what TPE do is to <strong>fit a probability distribution for the good observations (green circle) and one for the bad observations (orange cross)</strong> </p>
<blockquote>
<p>the second image shows these two distribution we fit. here EI denotes no longer normal EI but the promisingness we have defined using two distributions before</p>
</blockquote>
<p>we can find that the probability of the <strong>bad group is higher</strong> at the point where we have actually made observations (approximately x &#x3D; 0.5 in the second image), so we will <strong>not select history observations</strong> as the next new observation since in this iterate it’s <strong>promisingness is quite low</strong> due to the high probability under the bad distribution </p>
<img src="/2024/06/20/automl-paper-survey/image-20240827201109456.png" alt="image-20240827201109456" style="zoom:80%;">

<p>after we have <strong>chosen the configuration we want to evaluate next</strong> (maximum value of EI, approximately -0.1 in the image), we now again <strong>compute our good and bad group based on the value of gamma</strong> (re-evaluate which point belong to the good group and which point belongs to the bad one), and <strong>again fit the two distributions</strong>, then <strong>compute the expected improvement again</strong> </p>
<h2 id="types-of-distribution-used"><a href="#types-of-distribution-used" class="headerlink" title="types of distribution used"></a>types of distribution used</h2><p>depends on the <strong>type of the hyperparameter</strong></p>
<ul>
<li>categorical &#x3D;&gt; categorical distribution (point mass frequency)</li>
<li>real-valued&#x2F;integer<ul>
<li>sample in original space &#x3D;&gt; <strong>mixture</strong> of <strong>truncated</strong> Gaussians</li>
<li>sample in log-space &#x3D;&gt; <strong>mixture</strong> of <strong>truncated</strong> Gaussians in log-space<ul>
<li><u>what is the mixture really is</u></li>
<li><u>why should use truncated distribution</u> &#x3D;&gt; in the previous hyperparameter optimization as well as in random search we actually specify the admissible range of values over which we want to search and we want to be able to do the same for TPE, so we want to be able to say this is the minimum value or this is the maximum value and go find something in between. By truncating these distributions we can enforce this constraint so that the values outside of this range automatically get assigned a probability of zero</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="about-mixture"><a href="#about-mixture" class="headerlink" title="about mixture"></a>about mixture</h3><blockquote>
<p>suppose there are 3 observations</p>
</blockquote>
<p>what TPE does is that it’s going to fit a Gaussian to every one of these points. </p>
<ul>
<li>so every observation will get a Gaussian fitted to it <strong>with the mean equivalent to its hyperparameter value.</strong> </li>
<li>the <strong>standard deviations of these Gaussians set</strong> are equal to the <strong>maximum distance between</strong> the right or the left <strong>neighbor</strong></li>
</ul>
<p>after every value have their own different Gaussian distribution, now we can <strong>mix these distribution</strong> in order to end up with <strong>a single distribution</strong> </p>
<p>if we want to <strong>compute the mixture probability density at a point</strong>, we just need to first compute its density under each original Gaussian distribution and then normalize by the number of gaussians so that it stays a valid probability</p>
<h2 id="cons"><a href="#cons" class="headerlink" title="cons"></a>cons</h2><ul>
<li>easily supports mixed continuous and iscreate spaces due to the nature of kernel density estimators</li>
<li>model construction scales linearly in the number of data points in contrast to the cubic-time Gaussian processes predominant in the BO literature</li>
</ul>
<h1 id="BOHB"><a href="#BOHB" class="headerlink" title="BOHB"></a>BOHB</h1><ul>
<li><strong>vanilla bayesian optimization</strong> is typically computationally infeasible</li>
<li><strong>Bandit-bansed</strong> configuration evaluation approach <strong>based on random search</strong> lack guidance and do not converge to the best configurations as quickly</li>
</ul>
<p>what bohb to do is to <strong>combine</strong> the benefits of these two methods (in particular, hyperband)  </p>
<p>&#x3D;&gt; model-based hyper band, <u>strong anytime performance and fast convergence to optimal configurations</u></p>
<h2 id="idea-4"><a href="#idea-4" class="headerlink" title="idea"></a>idea</h2><p>BOHB relies on HB to determine how many configurations to evaluate with which budget, but it <strong>replaces the random selection of configurations</strong> at the beginning of each HB iteration <strong>by a model-based search</strong></p>
<ul>
<li>once the desired number of configurations for the ieration is reached, the standard successive halving procedure is carried out using these configurations.</li>
<li>we keep track of the performance of all function evaluations $g(x, b) + \epsilon$<ul>
<li>here $x$ denotes configurations</li>
<li>$b$ denotes the budget</li>
<li>and $\epsilon$ denotes the basis of the model</li>
</ul>
</li>
</ul>
<h2 id="algorithm"><a href="#algorithm" class="headerlink" title="algorithm"></a>algorithm</h2><p>closely resembles TPE, with only one major difference.</p>
<blockquote>
<p>Image below shows the initial configuration selection procedure of BOHB</p>
</blockquote>
<p>![截屏2024-09-01 23.03.55](.&#x2F;automl-paper-survey&#x2F;截屏2024-09-01 23.03.55.png)</p>
<p>here</p>
<ul>
<li><p>$d$ denotes the number of hyperparameter</p>
</li>
<li><p>$N_{min}$ denotes <strong>the minimum number of data points to build the selection model</strong>, in this paper the author set to $d+1$ for experiments</p>
</li>
<li><p>$\vert D_b \vert$ denotes the number of observations $D$ of the input for budget $b$</p>
</li>
<li><p>$l(x) &#x3D; KDEs$, $l’(x) &#x3D; b_w · l(x)$</p>
</li>
<li><p>Promisingness &#x3D; $\frac{l(x)}{g(x)}$</p>
</li>
</ul>
<p>Then</p>
<ul>
<li><p>we opt for <strong>a single multidimensional KDE</strong> compared to the hierarchy of one-dimensional KDEs used in TPE in order to better handle interaction effects in the input space</p>
<ul>
<li>so the <strong>keypoint here is to fit a useful KDEs</strong></li>
</ul>
</li>
<li><p>to build the model as early as possible, we do not wait until $N_b$ equal to the number of observations $\vert D_b \vert$ for budget $b$ which we input.</p>
<ul>
<li><p>instead, we initialize with $N_{min} + 2$ random configurations first</p>
</li>
<li><p>we set</p>
<ul>
<li>$N_{b, l} &#x3D; \max (N_{min}, q · N_b)$ where $q$ denotes the percentile which is the  input of the algorithm</li>
<li>$N_{b, g} &#x3D; \max (N_{min}, N_b - N_{b, l})$</li>
<li>here the <strong>first one denotes number of best configurations</strong> and the second denotes the worst one, since we use <strong>TPE method</strong> as the BO part </li>
<li><u>this step is equal to re-evaluating good points and bad points in TPE</u></li>
</ul>
</li>
<li><p>then we fit the KDEs ( $l(x)$, $g(x)$ ) according to the TPE method and $N_{b, l}, N_{b, g}$</p>
<ul>
<li><u>this step is equal to model the densities (fit two distribution) in TPE</u></li>
</ul>
</li>
<li><p>after we fit the $l(x)$,  we need to determine the configurations sampling parameter according to $l(x)$, we do not use $l(x)$ directly, instead</p>
<ul>
<li>we multiply it by a factor $b_w$ to encourage more exploration and we call it $l’(x)$ </li>
<li>then we sample $N_s$ points from $l’(x)$ as the sampling result</li>
<li>keep track of the promisingness and its sampling result of this iteration</li>
</ul>
</li>
<li><p><strong>return</strong> the sampling results with highest ratio (promisingness)</p>
</li>
</ul>
</li>
</ul>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><ol>
<li>AutoML: A survey of the state-of-the-art</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/139605200">https://zhuanlan.zhihu.com/p/139605200</a></li>
<li>Bergstra, J., Bardenet, R., Bengio, Y., and Kégl, B. Algorithms  for hyper-parameter optimization. In Advances in Neural Information  Processing Systems, 2011.</li>
<li>Hutter, F., Hoos, H., and Leyton-Brown, K. Sequential modelbased  optimization for general algorithm conﬁguration. In Learning and  Intelligent Optimization, volume 6683, 2011.</li>
<li>D.R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global Optimization, 21:345–383, 2001.</li>
<li>J. Villemonteix, E. Vazquez, and E. Walter. An informational approach to the global optimization of expensive-to-evaluate functions. Journal of Global Optimization, 2006.</li>
<li>N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In ICML, 2010.</li>
<li>A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning</li>
<li>amieson, K. and Talwalkar, A. Non-stochastic best arm  identiﬁcation and hyperparameter optimization. In Conference on  Artiﬁcial Intelligence and Statistics, 2016.</li>
<li>Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and  Talwalkar, A. Hyperband: Bandit-based conﬁguration evaluation for  hyperparameter optimization. In International Conference on Learning  Representations, 2017.</li>
</ol>

    </article>
    <!-- license -->
    
    <!-- paginator -->
    <ul class="post-paginator">
        <li class="next">
            
                <div class="nextSlogan">Next Post</div>
                <a href="/2024/09/05/knob-tunning/" title="knob-tuning">
                    <div class="nextTitle">knob-tuning</div>
                </a>
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href="/2024/03/18/algorithm-backtracking/" title="回溯">
                    <div class="prevTitle">回溯</div>
                </a>
            
        </li>
    </ul>
    <!-- comment -->
    
        <div class="post-comment">
            <!-- 来必力 City 版安装代码 -->


            

            

            

            <!-- utteranc评论 -->


            <!-- partial('_partial/comment/changyan') -->
            <!--PC版-->


            
            

            

        </div>
    
    <!-- timeliness note -->
    <!-- idea from: https://hexo.fluid-dev.com/posts/hexo-injector/#%E6%96%87%E7%AB%A0%E6%97%B6%E6%95%88%E6%80%A7%E6%8F%90%E7%A4%BA -->
    
    <!-- Mathjax -->
    
        
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
        };
    </script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>


    
</main>

                <!-- profile -->
                
            </div>
            <footer class="footer footer-unloaded">
    <!-- social  -->
    
        <div class="social">
            
    
        
            
                <a href="mailto:elubrazione@gmail.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/Elubrazione" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
            
                <a href="https://www.instagram.com/lingchingram/" class="iconfont-archer instagram" target="_blank" title=instagram></a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    


        </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info"><a target="_blank">Elubrazione</a></span>
    </div>
    <!-- website approve for Chinese user -->
    
    <!-- 不蒜子  -->
    
        <div class="busuanzi-container">
            
             
                <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
            
        </div>
    	
</footer>

        </div>
        <!-- toc -->
        
            <div class="toc-wrapper toc-wrapper-loding" style=







    top:50vh;

>
                <div class="toc-catalog">
                    <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
                </div>
                <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#machine-learning"><span class="toc-number">1.1.</span> <span class="toc-text">machine learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#role-of-the-human-experts"><span class="toc-number">1.2.</span> <span class="toc-text">role of the human experts</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ml-x3D-gt-automl"><span class="toc-number">1.3.</span> <span class="toc-text">ml &#x3D;&gt; automl</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#why-automate-ml"><span class="toc-number">1.4.</span> <span class="toc-text">why automate ml</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#auto-ml"><span class="toc-number">1.5.</span> <span class="toc-text">auto ml</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#overview"><span class="toc-number">1.6.</span> <span class="toc-text">overview</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CASH-vs-HPO"><span class="toc-number">2.</span> <span class="toc-text">CASH vs HPO</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#definition-of-CASH"><span class="toc-number">2.1.</span> <span class="toc-text">definition of CASH</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#definition-of-HPO"><span class="toc-number">2.2.</span> <span class="toc-text">definition of HPO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#relationship"><span class="toc-number">2.3.</span> <span class="toc-text">relationship</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GS-vs-RS"><span class="toc-number">3.</span> <span class="toc-text">GS vs RS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#grid-search"><span class="toc-number">3.1.</span> <span class="toc-text">grid search</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#definition"><span class="toc-number">3.1.1.</span> <span class="toc-text">definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pros-and-cons"><span class="toc-number">3.1.2.</span> <span class="toc-text">pros and cons</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#random-search"><span class="toc-number">3.2.</span> <span class="toc-text">random search</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#definition-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pros-and-cons-1"><span class="toc-number">3.2.2.</span> <span class="toc-text">pros and cons</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#relationship-1"><span class="toc-number">3.3.</span> <span class="toc-text">relationship</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Successive-Halving-vs-Hyperband"><span class="toc-number">4.</span> <span class="toc-text">Successive Halving vs Hyperband</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#successive-halving"><span class="toc-number">4.1.</span> <span class="toc-text">successive halving</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#idea"><span class="toc-number">4.1.1.</span> <span class="toc-text">idea</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#definition-2"><span class="toc-number">4.1.2.</span> <span class="toc-text">definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hyperparameters"><span class="toc-number">4.1.3.</span> <span class="toc-text">hyperparameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pros-and-cons-2"><span class="toc-number">4.1.4.</span> <span class="toc-text">pros and cons</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hyperband"><span class="toc-number">4.2.</span> <span class="toc-text">hyperband</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#idea-1"><span class="toc-number">4.2.1.</span> <span class="toc-text">idea</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#definition-3"><span class="toc-number">4.2.2.</span> <span class="toc-text">definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hyperparameters-1"><span class="toc-number">4.2.3.</span> <span class="toc-text">hyperparameters</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SMBO"><span class="toc-number">5.</span> <span class="toc-text">SMBO</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#limitation-of-techinique-so-far"><span class="toc-number">5.1.</span> <span class="toc-text">limitation of techinique so far</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#idea-2"><span class="toc-number">5.2.</span> <span class="toc-text">idea</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#instantiation"><span class="toc-number">5.3.</span> <span class="toc-text">instantiation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#surrogate-model"><span class="toc-number">5.4.</span> <span class="toc-text">surrogate model</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BO"><span class="toc-number">6.</span> <span class="toc-text">BO</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E4%B8%8E%E5%85%B6%E5%87%BD%E6%95%B0"><span class="toc-number">6.1.</span> <span class="toc-text">似然估计与其函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#problem-description"><span class="toc-number">6.2.</span> <span class="toc-text">problem description</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#iterating-method"><span class="toc-number">6.3.</span> <span class="toc-text">iterating method</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#algorithm-procedure"><span class="toc-number">6.4.</span> <span class="toc-text">algorithm procedure</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GP"><span class="toc-number">7.</span> <span class="toc-text">GP</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#probability-modeling"><span class="toc-number">7.1.</span> <span class="toc-text">probability modeling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%BB%B4%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E7%9A%84%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0"><span class="toc-number">7.1.1.</span> <span class="toc-text">多维正态分布的似然函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#process"><span class="toc-number">7.1.2.</span> <span class="toc-text">process</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#final-modeling"><span class="toc-number">7.1.3.</span> <span class="toc-text">final modeling</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#next-observation"><span class="toc-number">7.2.</span> <span class="toc-text">next observation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#EI"><span class="toc-number">7.2.1.</span> <span class="toc-text">EI</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#figure-example"><span class="toc-number">7.3.</span> <span class="toc-text">figure example</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pros"><span class="toc-number">7.4.</span> <span class="toc-text">pros</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SMAC"><span class="toc-number">8.</span> <span class="toc-text">SMAC</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#idea-3"><span class="toc-number">8.1.</span> <span class="toc-text">idea</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#modeling-process"><span class="toc-number">8.2.</span> <span class="toc-text">modeling process</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#next-observation-1"><span class="toc-number">8.3.</span> <span class="toc-text">next observation</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#TPE"><span class="toc-number">9.</span> <span class="toc-text">TPE</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#definition-4"><span class="toc-number">9.1.</span> <span class="toc-text">definition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#example"><span class="toc-number">9.2.</span> <span class="toc-text">example</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#types-of-distribution-used"><span class="toc-number">9.3.</span> <span class="toc-text">types of distribution used</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#about-mixture"><span class="toc-number">9.3.1.</span> <span class="toc-text">about mixture</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cons"><span class="toc-number">9.4.</span> <span class="toc-text">cons</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BOHB"><span class="toc-number">10.</span> <span class="toc-text">BOHB</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#idea-4"><span class="toc-number">10.1.</span> <span class="toc-text">idea</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#algorithm"><span class="toc-number">10.2.</span> <span class="toc-text">algorithm</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#references"><span class="toc-number">11.</span> <span class="toc-text">references</span></a></li></ol>
            </div>
        
        <!-- sidebar -->
        <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
        <div class="sidebar-panel-archives">
    <!-- 在 ejs 中将 archive 按照时间排序 -->
    
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 67
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
        
            
            
            <div class="archive-year"> 2024 </div>
            <ul class="year-list">
            
        
        <li class="archive-post-item">
            <span class="archive-post-date">09/05</span>
            <a class="archive-post-title" href="/2024/09/05/knob-tunning/">knob-tuning</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">06/20</span>
            <a class="archive-post-title" href="/2024/06/20/automl-paper-survey/">automl-paper-survey</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">03/18</span>
            <a class="archive-post-title" href="/2024/03/18/algorithm-backtracking/">回溯</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">03/14</span>
            <a class="archive-post-title" href="/2024/03/14/algorithm-mst/">最小生成树</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">03/09</span>
            <a class="archive-post-title" href="/2024/03/09/algorithm-dp-stock/">动态规划股票买卖问题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">03/06</span>
            <a class="archive-post-title" href="/2024/03/06/algorithm-stack-and-queue/">栈与队列</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">02/29</span>
            <a class="archive-post-title" href="/2024/02/29/algorithm-two-pointers/">双指针</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">02/07</span>
            <a class="archive-post-title" href="/2024/02/07/algorithm-dp-bag/">动态规划背包问题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">01/16</span>
            <a class="archive-post-title" href="/2024/01/16/algorithm-dynamic-programming/">动态规划子序列问题</a>
        </li>
    
        
            
            
                
                </ul>
            
            <div class="archive-year"> 2023 </div>
            <ul class="year-list">
            
        
        <li class="archive-post-item">
            <span class="archive-post-date">12/31</span>
            <a class="archive-post-title" href="/2023/12/31/essay-3/">2023年末总结</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">12/17</span>
            <a class="archive-post-title" href="/2023/12/17/pa-virtual-machine/">HUST系统能力培养之虚拟机</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/27</span>
            <a class="archive-post-title" href="/2023/11/27/cs224w-heterogeneous-graphs/">heterogeneous graphs</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/18</span>
            <a class="archive-post-title" href="/2023/11/18/idsm-env-problems/">Greenplum环境问题合集</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/12</span>
            <a class="archive-post-title" href="/2023/11/12/devign/">HUST图神经网络Devign模型</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/11</span>
            <a class="archive-post-title" href="/2023/11/11/cs224w-graph-neural-network/">graph neural network</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/10</span>
            <a class="archive-post-title" href="/2023/11/10/cs224w-message-passing-and-node-classification/">message passing and node classification</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/09</span>
            <a class="archive-post-title" href="/2023/11/09/cs224w-node-embedding/">node embeddings</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/07</span>
            <a class="archive-post-title" href="/2023/11/07/cs224w-traditional-feature-based-methods/">traditional feature-based methods</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">09/13</span>
            <a class="archive-post-title" href="/2023/09/13/idsm-tsunami-indexes/">Tsunami：a learned multi-dimensional index for correlated data and skewed workloads</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">09/11</span>
            <a class="archive-post-title" href="/2023/09/11/pat-2023-summer/">PAT甲级2023夏真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">09/10</span>
            <a class="archive-post-title" href="/2023/09/10/pat-2018-spring/">PAT甲级2018春真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">09/09</span>
            <a class="archive-post-title" href="/2023/09/09/pat-2017-winter/">PAT甲级2017冬真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">09/07</span>
            <a class="archive-post-title" href="/2023/09/07/pat-2017-fall/">PAT甲级2017秋真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">09/05</span>
            <a class="archive-post-title" href="/2023/09/05/pat-linked-note/">PAT链表笔记</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">09/03</span>
            <a class="archive-post-title" href="/2023/09/03/pat-2017-summer/">PAT甲级2017夏真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span>
            <a class="archive-post-title" href="/2023/09/02/pat-primarily-note/">PAT算法杂烩笔记</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">09/01</span>
            <a class="archive-post-title" href="/2023/09/01/pat-2020-fall/">PAT甲级2020秋真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/31</span>
            <a class="archive-post-title" href="/2023/08/31/pat-2020-spring/">PAT甲级2020春真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/31</span>
            <a class="archive-post-title" href="/2023/08/31/pat-2017-spring/">PAT甲级2017春真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/30</span>
            <a class="archive-post-title" href="/2023/08/30/idsm-learning-multi-dimensional-indexes/">Learning Multi-dimensional Indexes</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/29</span>
            <a class="archive-post-title" href="/2023/08/29/pat-2016-winter/">PAT甲级2016冬真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/26</span>
            <a class="archive-post-title" href="/2023/08/26/pat-2016-fall/">PAT甲级2016秋真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/26</span>
            <a class="archive-post-title" href="/2023/08/26/pat-union-find-set-note/">PAT并查集笔记</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/25</span>
            <a class="archive-post-title" href="/2023/08/25/pat-2016-summer/">PAT甲级2016夏真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/21</span>
            <a class="archive-post-title" href="/2023/08/21/pat-2016-spring/">PAT甲级2016春真题</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/20</span>
            <a class="archive-post-title" href="/2023/08/20/pat-graph-note/">PAT图笔记</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/19</span>
            <a class="archive-post-title" href="/2023/08/19/idsm-z-order/">Z-Order</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/13</span>
            <a class="archive-post-title" href="/2023/08/13/pat-tree-note/">PAT树笔记</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">07/26</span>
            <a class="archive-post-title" href="/2023/07/26/idsm-env-debugs/">Greenplum操作遇到的问题记录</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">06/29</span>
            <a class="archive-post-title" href="/2023/06/29/idsm-env-installation/">Greenplum & PostgreSQL安装记录</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">04/30</span>
            <a class="archive-post-title" href="/2023/04/30/3d-reconstruction/">HUST基于深度学习的三维重建</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">04/26</span>
            <a class="archive-post-title" href="/2023/04/26/bigdata-management-4/">HUST大数据管理lab4教程</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">04/13</span>
            <a class="archive-post-title" href="/2023/04/13/bigdata-management-3/">HUST大数据管理lab3教程</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">04/04</span>
            <a class="archive-post-title" href="/2023/04/04/idsm-slalom/">Slalom：Costing Through Raw Data via Adaptive Partitioning and Indexing</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">03/24</span>
            <a class="archive-post-title" href="/2023/03/24/bigdata-management-2/">HUST大数据管理lab2教程</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">03/12</span>
            <a class="archive-post-title" href="/2023/03/12/bigdata-management-1/">HUST大数据管理lab1教程</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">01/22</span>
            <a class="archive-post-title" href="/2023/01/22/idsm-automap/">AutoMAP：Diagnose Your Microservice-based Web Applications Automatically</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">01/19</span>
            <a class="archive-post-title" href="/2023/01/19/idsm-lifelong-disk-failure-prediction/">Lifelong Disk Failure Prediction via GAN-based Anomaly Detection</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">01/15</span>
            <a class="archive-post-title" href="/2023/01/15/idsm-minority-disk-failure-prediction/">Minority Disk Failure Prediction Based on Transfer Learning in Large Data Centers of Heterogeneous Disk Systems</a>
        </li>
    
        
            
            
                
                </ul>
            
            <div class="archive-year"> 2022 </div>
            <ul class="year-list">
            
        
        <li class="archive-post-item">
            <span class="archive-post-date">11/20</span>
            <a class="archive-post-title" href="/2022/11/20/idsm-git-tool/">Git其它用法补充</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">10/22</span>
            <a class="archive-post-title" href="/2022/10/22/react-note/">React笔记</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">10/17</span>
            <a class="archive-post-title" href="/2022/10/17/react-refs-and-dom/">Refs and the DOM</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">10/16</span>
            <a class="archive-post-title" href="/2022/10/16/react-webpack/">Webpack</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">08/04</span>
            <a class="archive-post-title" href="/2022/08/04/essay-2/">七月的最后一个周末</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">07/29</span>
            <a class="archive-post-title" href="/2022/07/29/leetcode-18/">18.四数之和</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">07/27</span>
            <a class="archive-post-title" href="/2022/07/27/leetcode-142/">142.环形链表Ⅱ</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">07/24</span>
            <a class="archive-post-title" href="/2022/07/24/leetcode-15/">15.三数之和</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">06/26</span>
            <a class="archive-post-title" href="/2022/06/26/dl-transformer/">预训练语言模型</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">06/10</span>
            <a class="archive-post-title" href="/2022/06/10/essay-1/">如若还能下一场2018年的银杏雨</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">05/12</span>
            <a class="archive-post-title" href="/2022/05/12/ml-ensemble-learning/">机器学习-集成学习</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">04/27</span>
            <a class="archive-post-title" href="/2022/04/27/ml-support-vector-machine/">机器学习-支持向量机</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">04/23</span>
            <a class="archive-post-title" href="/2022/04/23/ml-decision-tree/">机器学习-决策树</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">04/16</span>
            <a class="archive-post-title" href="/2022/04/16/ml-linear-model/">机器学习-线性模型</a>
        </li>
    
        
            
            
                
                </ul>
            
            <div class="archive-year"> 2021 </div>
            <ul class="year-list">
            
        
        <li class="archive-post-item">
            <span class="archive-post-date">04/15</span>
            <a class="archive-post-title" href="/2021/04/15/algorithm-finding/">数据结构-查找</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">03/30</span>
            <a class="archive-post-title" href="/2021/03/30/virtual-env-install/">虚拟机/服务器基础环境配置</a>
        </li>
    
        
        <li class="archive-post-item">
            <span class="archive-post-date">01/05</span>
            <a class="archive-post-title" href="/2021/01/05/english-toefl-subjects-words/">托福学科单词</a>
        </li>
    
        
            
            
                
                </ul>
            
            <div class="archive-year"> 2020 </div>
            <ul class="year-list">
            
        
        <li class="archive-post-item">
            <span class="archive-post-date">12/22</span>
            <a class="archive-post-title" href="/2020/12/22/welcome/">欢迎来到我的博客</a>
        </li>
    
    </div>
</div>

        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
        
            <span class="sidebar-tag-name" data-tags="动态规划">
                <span class="iconfont-archer">&#xe606;</span>
                动态规划
            </span>
        
            <span class="sidebar-tag-name" data-tags="查找">
                <span class="iconfont-archer">&#xe606;</span>
                查找
            </span>
        
            <span class="sidebar-tag-name" data-tags="教程">
                <span class="iconfont-archer">&#xe606;</span>
                教程
            </span>
        
            <span class="sidebar-tag-name" data-tags="MongoDB">
                <span class="iconfont-archer">&#xe606;</span>
                MongoDB
            </span>
        
            <span class="sidebar-tag-name" data-tags="数据库">
                <span class="iconfont-archer">&#xe606;</span>
                数据库
            </span>
        
            <span class="sidebar-tag-name" data-tags="Neo4j">
                <span class="iconfont-archer">&#xe606;</span>
                Neo4j
            </span>
        
            <span class="sidebar-tag-name" data-tags="MySQL">
                <span class="iconfont-archer">&#xe606;</span>
                MySQL
            </span>
        
            <span class="sidebar-tag-name" data-tags="机器学习">
                <span class="iconfont-archer">&#xe606;</span>
                机器学习
            </span>
        
            <span class="sidebar-tag-name" data-tags="GNN">
                <span class="iconfont-archer">&#xe606;</span>
                GNN
            </span>
        
            <span class="sidebar-tag-name" data-tags="随笔">
                <span class="iconfont-archer">&#xe606;</span>
                随笔
            </span>
        
            <span class="sidebar-tag-name" data-tags="故障诊断">
                <span class="iconfont-archer">&#xe606;</span>
                故障诊断
            </span>
        
            <span class="sidebar-tag-name" data-tags="Greenplum操作">
                <span class="iconfont-archer">&#xe606;</span>
                Greenplum操作
            </span>
        
            <span class="sidebar-tag-name" data-tags="Linux磁盘扩容">
                <span class="iconfont-archer">&#xe606;</span>
                Linux磁盘扩容
            </span>
        
            <span class="sidebar-tag-name" data-tags="debug记录">
                <span class="iconfont-archer">&#xe606;</span>
                debug记录
            </span>
        
            <span class="sidebar-tag-name" data-tags="环境问题">
                <span class="iconfont-archer">&#xe606;</span>
                环境问题
            </span>
        
            <span class="sidebar-tag-name" data-tags="多维索引">
                <span class="iconfont-archer">&#xe606;</span>
                多维索引
            </span>
        
            <span class="sidebar-tag-name" data-tags="磁盘故障">
                <span class="iconfont-archer">&#xe606;</span>
                磁盘故障
            </span>
        
            <span class="sidebar-tag-name" data-tags="迁移学习">
                <span class="iconfont-archer">&#xe606;</span>
                迁移学习
            </span>
        
            <span class="sidebar-tag-name" data-tags="列存分区">
                <span class="iconfont-archer">&#xe606;</span>
                列存分区
            </span>
        
            <span class="sidebar-tag-name" data-tags="链表">
                <span class="iconfont-archer">&#xe606;</span>
                链表
            </span>
        
            <span class="sidebar-tag-name" data-tags="LeetCode">
                <span class="iconfont-archer">&#xe606;</span>
                LeetCode
            </span>
        
            <span class="sidebar-tag-name" data-tags="双指针">
                <span class="iconfont-archer">&#xe606;</span>
                双指针
            </span>
        
            <span class="sidebar-tag-name" data-tags="剪枝">
                <span class="iconfont-archer">&#xe606;</span>
                剪枝
            </span>
        
            <span class="sidebar-tag-name" data-tags="素数">
                <span class="iconfont-archer">&#xe606;</span>
                素数
            </span>
        
            <span class="sidebar-tag-name" data-tags="逻辑题">
                <span class="iconfont-archer">&#xe606;</span>
                逻辑题
            </span>
        
            <span class="sidebar-tag-name" data-tags="并查集">
                <span class="iconfont-archer">&#xe606;</span>
                并查集
            </span>
        
            <span class="sidebar-tag-name" data-tags="树的遍历">
                <span class="iconfont-archer">&#xe606;</span>
                树的遍历
            </span>
        
            <span class="sidebar-tag-name" data-tags="哈希">
                <span class="iconfont-archer">&#xe606;</span>
                哈希
            </span>
        
            <span class="sidebar-tag-name" data-tags="字符串">
                <span class="iconfont-archer">&#xe606;</span>
                字符串
            </span>
        
            <span class="sidebar-tag-name" data-tags="排序">
                <span class="iconfont-archer">&#xe606;</span>
                排序
            </span>
        
            <span class="sidebar-tag-name" data-tags="二叉搜索树">
                <span class="iconfont-archer">&#xe606;</span>
                二叉搜索树
            </span>
        
            <span class="sidebar-tag-name" data-tags="Set">
                <span class="iconfont-archer">&#xe606;</span>
                Set
            </span>
        
            <span class="sidebar-tag-name" data-tags="图论">
                <span class="iconfont-archer">&#xe606;</span>
                图论
            </span>
        
            <span class="sidebar-tag-name" data-tags="AVL树">
                <span class="iconfont-archer">&#xe606;</span>
                AVL树
            </span>
        
            <span class="sidebar-tag-name" data-tags="数学模拟">
                <span class="iconfont-archer">&#xe606;</span>
                数学模拟
            </span>
        
            <span class="sidebar-tag-name" data-tags="贪心">
                <span class="iconfont-archer">&#xe606;</span>
                贪心
            </span>
        
            <span class="sidebar-tag-name" data-tags="DFS">
                <span class="iconfont-archer">&#xe606;</span>
                DFS
            </span>
        
            <span class="sidebar-tag-name" data-tags="图模拟">
                <span class="iconfont-archer">&#xe606;</span>
                图模拟
            </span>
        
            <span class="sidebar-tag-name" data-tags="拓扑排序">
                <span class="iconfont-archer">&#xe606;</span>
                拓扑排序
            </span>
        
            <span class="sidebar-tag-name" data-tags="最短路径">
                <span class="iconfont-archer">&#xe606;</span>
                最短路径
            </span>
        
            <span class="sidebar-tag-name" data-tags="快乐模拟">
                <span class="iconfont-archer">&#xe606;</span>
                快乐模拟
            </span>
        
            <span class="sidebar-tag-name" data-tags="栈">
                <span class="iconfont-archer">&#xe606;</span>
                栈
            </span>
        
            <span class="sidebar-tag-name" data-tags="递推">
                <span class="iconfont-archer">&#xe606;</span>
                递推
            </span>
        
            <span class="sidebar-tag-name" data-tags="前端">
                <span class="iconfont-archer">&#xe606;</span>
                前端
            </span>
        
            <span class="sidebar-tag-name" data-tags="React">
                <span class="iconfont-archer">&#xe606;</span>
                React
            </span>
        
            <span class="sidebar-tag-name" data-tags="Webpack">
                <span class="iconfont-archer">&#xe606;</span>
                Webpack
            </span>
        
            <span class="sidebar-tag-name" data-tags="Eslint">
                <span class="iconfont-archer">&#xe606;</span>
                Eslint
            </span>
        
            <span class="sidebar-tag-name" data-tags="top">
                <span class="iconfont-archer">&#xe606;</span>
                top
            </span>
        
            <span class="sidebar-tag-name" data-tags="完全二叉树">
                <span class="iconfont-archer">&#xe606;</span>
                完全二叉树
            </span>
        
            <span class="sidebar-tag-name" data-tags="图的遍历">
                <span class="iconfont-archer">&#xe606;</span>
                图的遍历
            </span>
        
            <span class="sidebar-tag-name" data-tags="计算机视觉">
                <span class="iconfont-archer">&#xe606;</span>
                计算机视觉
            </span>
        
            <span class="sidebar-tag-name" data-tags="三维重建">
                <span class="iconfont-archer">&#xe606;</span>
                三维重建
            </span>
        
            <span class="sidebar-tag-name" data-tags="深度学习">
                <span class="iconfont-archer">&#xe606;</span>
                深度学习
            </span>
        
            <span class="sidebar-tag-name" data-tags="托福">
                <span class="iconfont-archer">&#xe606;</span>
                托福
            </span>
        
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
        缺失模块，请参考主题文档进行安装配置：https://github.com/fi3ework/hexo-theme-archer#%E5%AE%89%E8%A3%85%E4%B8%BB%E9%A2%98
    </div> 
    <div class="sidebar-tags-list"></div>
</div>

        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="算法">
            <span class="iconfont-archer">&#xe60a;</span>
            算法
        </span>
    
        <span class="sidebar-category-name" data-categories="华科课程">
            <span class="iconfont-archer">&#xe60a;</span>
            华科课程
        </span>
    
        <span class="sidebar-category-name" data-categories="学习笔记">
            <span class="iconfont-archer">&#xe60a;</span>
            学习笔记
        </span>
    
        <span class="sidebar-category-name" data-categories="年末总结">
            <span class="iconfont-archer">&#xe60a;</span>
            年末总结
        </span>
    
        <span class="sidebar-category-name" data-categories="腾讯AI训练营">
            <span class="iconfont-archer">&#xe60a;</span>
            腾讯AI训练营
        </span>
    
        <span class="sidebar-category-name" data-categories="英语">
            <span class="iconfont-archer">&#xe60a;</span>
            英语
        </span>
    
        <span class="sidebar-category-name" data-categories="database">
            <span class="iconfont-archer">&#xe60a;</span>
            database
        </span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>

    </div>
</div>

        <!-- site-meta -->
        <script>
    var siteMetaRoot = "/"
    if (siteMetaRoot === "undefined") {
        siteMetaRoot = '/'
    }
    var siteMeta = {
        url: "https://elubrazione.github.io",
        root: siteMetaRoot,
        author: "Elubrazione"
    }
</script>

        <!-- import experimental options here -->
        <!-- Custom Font -->

    <!-- Check browser compatibility of CSS variables -->
    <script>
        if (browserSupportCSSVariables === undefined) {
            var browserSupportCSSVariables = window.CSS && window.CSS.supports && window.CSS.supports('--a', 0);
        }
    </script>
    <script>
        if (browserSupportCSSVariables) {
            var customFontName = 'Noto Sans SC:n3,n4,n5,n7'
            var customFontUrl = 'https://fonts.googleapis.cnpmjs.org/css2?family=Noto+Sans+SC:wght@300;400;500;700&amp;display=swap'
            if (!customFontName) {
                console.log('Custom font name is not set or read failed');
            }
            if (!customFontUrl) {
                console.log('Custom font url is not set or read failed');
            }
        } else {
            console.error('Current browser doesn\'t support custom font.')
        }
    </script>
    <script src="/scripts/customFontLoader.js?v=20211217" defer></script>


        <!-- main func -->
        <script src="/scripts/main.js?v=20211217"></script>
        <!-- dark mode -->
        <script src="/scripts/dark.js?v=20211217"></script>
        <!-- fancybox -->
        <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" defer></script>
        <!-- algolia -->
        
        <!-- busuanzi -->
        
            <script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>
        
        <!-- CNZZ -->
        
        <!-- async load share.js -->
        
            <script src="/scripts/share.js?v=20211217" async></script>
        
        <!-- mermaid -->
        
            <script src='https://cdn.jsdelivr.net/npm/mermaid@8.11.0/dist/mermaid.min.js'></script>
            <script>
                if (window.mermaid) {
                    mermaid.initialize({theme: 'dark'});
                }
            </script>
        
    </body>
</html>
